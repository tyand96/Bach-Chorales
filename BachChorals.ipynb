{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "from midiutil import MIDIFile\n",
    "from io import BytesIO\n",
    "import pygame\n",
    "import pygame.mixer\n",
    "from time import sleep\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = tf.keras.utils.get_file(\n",
    "    origin=\"https://homl.info/bach\",\n",
    "    cache_dir=\".\",\n",
    "    extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_VAL = 36 # Smallest note value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bach_dataset(\n",
    "    dataset_type: str,\n",
    "    window_size: int=16,\n",
    "    window_shift: int=1,\n",
    "    cache: bool=False,\n",
    "    shuffle_buffer_size: int=None,\n",
    "    batch_size: int=32,\n",
    "    seed: int=42\n",
    ") -> tf.data.Dataset:\n",
    "    root_dir = Path(\"./datasets/jsb_chorales\")\n",
    "    filepaths = sorted([str(path) for path in (root_dir / dataset_type).glob(\"chorale_*.csv\")])\n",
    "    min_val = 36 # smallest chord value\n",
    "\n",
    "    def read_file(chorale_file_path: str) -> tf.data.Dataset:\n",
    "        types = [int(), int(), int(), int()]\n",
    "        return tf.data.experimental.CsvDataset(chorale_file_path, record_defaults=types, header=True)\n",
    "    \n",
    "    def group_notes(*notes: tf.Tensor) -> tf.Tensor:\n",
    "        return tf.stack(notes, axis=-1)\n",
    "    \n",
    "    def create_arpegio(chord_batch: tf.Tensor) -> tf.Tensor:\n",
    "        # First, rescale notes\n",
    "        chord_batch = tf.where(chord_batch == 0, chord_batch, chord_batch - min_val + 1)\n",
    "        arpegio = tf.reshape(chord_batch, [-1])\n",
    "        return arpegio\n",
    "\n",
    "    dataset_list = []\n",
    "    for chorale_file in filepaths:\n",
    "        ds = read_file(chorale_file)\n",
    "        ds = ds.map(group_notes)\n",
    "        ds = ds.window(size=window_size+1, shift=window_shift, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda window_ds: window_ds.batch(window_size+1))\n",
    "        ds = ds.map(create_arpegio)\n",
    "\n",
    "        dataset_list.append(ds)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset_list)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda x: x,\n",
    "        cycle_length=1,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size is not None:\n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size, seed=seed)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda S: (S[:, :-1], S[:, 1:]))\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = bach_dataset(\"train\", cache=True, shuffle_buffer_size=1000, seed=42)\n",
    "valid_ds = bach_dataset(\"valid\", cache=True)\n",
    "test_ds = bach_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "        n_conv_layers:int=1, n_starting_filters:int=32, max_dilations:int=4,\n",
    "        n_recurrent_layers:int=1\n",
    "):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=[None]),\n",
    "        tf.keras.layers.Embedding(input_dim=47, output_dim=5)\n",
    "    ])\n",
    "\n",
    "    # Convolutional Layers\n",
    "    n_filters = n_starting_filters\n",
    "    for conv_idx in range(n_conv_layers):\n",
    "        dilation_rate = 2**(conv_idx % max_dilations)\n",
    "        # n_filters = (2**conv_idx) * n_starting_filters\n",
    "        model.add(\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=n_starting_filters,\n",
    "                kernel_size=2, padding=\"causal\",\n",
    "                activation=\"relu\", dilation_rate=dilation_rate\n",
    "            )\n",
    "        )\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # Recurrent layers\n",
    "    for rec_idx in range(n_recurrent_layers):\n",
    "        model.add(tf.keras.layers.GRU(n_starting_filters, return_sequences=True))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(47, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 5)           235       \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, None, 32)          352       \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, None, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, None, 32)          2080      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, None, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, None, 32)          2080      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, None, 32)          128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, None, 32)          2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, None, 32)          128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, None, 32)          6336      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 47)          1551      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15226 (59.48 KB)\n",
      "Trainable params: 14970 (58.48 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(n_conv_layers=4, n_recurrent_layers=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Nadam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=\"accuracy\"\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1612/1612 [==============================] - 33s 19ms/step - loss: 1.0605 - accuracy: 0.7331 - val_loss: 0.9131 - val_accuracy: 0.7594\n",
      "Epoch 2/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.7315 - accuracy: 0.7970 - val_loss: 0.8946 - val_accuracy: 0.7582\n",
      "Epoch 3/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6884 - accuracy: 0.8053 - val_loss: 0.8524 - val_accuracy: 0.7674\n",
      "Epoch 4/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6646 - accuracy: 0.8100 - val_loss: 0.8263 - val_accuracy: 0.7736\n",
      "Epoch 5/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6483 - accuracy: 0.8134 - val_loss: 0.8381 - val_accuracy: 0.7710\n",
      "Epoch 6/20\n",
      "1612/1612 [==============================] - 31s 19ms/step - loss: 0.6372 - accuracy: 0.8156 - val_loss: 0.8210 - val_accuracy: 0.7748\n",
      "Epoch 7/20\n",
      "1612/1612 [==============================] - 32s 20ms/step - loss: 0.6287 - accuracy: 0.8174 - val_loss: 0.8072 - val_accuracy: 0.7773\n",
      "Epoch 8/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6227 - accuracy: 0.8186 - val_loss: 0.8068 - val_accuracy: 0.7777\n",
      "Epoch 9/20\n",
      "1612/1612 [==============================] - 31s 19ms/step - loss: 0.6167 - accuracy: 0.8199 - val_loss: 0.7982 - val_accuracy: 0.7798\n",
      "Epoch 10/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6121 - accuracy: 0.8207 - val_loss: 0.8079 - val_accuracy: 0.7759\n",
      "Epoch 11/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6087 - accuracy: 0.8212 - val_loss: 0.8041 - val_accuracy: 0.7771\n",
      "Epoch 12/20\n",
      "1612/1612 [==============================] - 30s 19ms/step - loss: 0.6053 - accuracy: 0.8218 - val_loss: 0.8048 - val_accuracy: 0.7774\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, epochs=20,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hyper_model(hp: kt.HyperParameters):\n",
    "    n_conv_layers = hp.Int(\"n_conv_layers\", min_value=1, max_value=5)\n",
    "    n_recurrent_layers = hp.Int(\"n_recurrent_layers\", min_value=1, max_value=2)\n",
    "    \n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-7, max_value=1.0, sampling=\"log\")\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.Nadam(learning_rate=learning_rate)\n",
    "\n",
    "    model = build_model(\n",
    "        n_conv_layers=n_conv_layers, n_starting_filters=32,\n",
    "        max_dilations=4, n_recurrent_layers=n_recurrent_layers\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 23s]\n",
      "val_accuracy: 0.7042490839958191\n",
      "\n",
      "Best val_accuracy So Far: 0.7241138219833374\n",
      "Total elapsed time: 00h 04m 12s\n"
     ]
    }
   ],
   "source": [
    "baysian_opt_tuner = kt.BayesianOptimization(\n",
    "    build_hyper_model,\n",
    "    objective=\"val_accuracy\", seed=42,\n",
    "    max_trials=10,\n",
    "    overwrite=True, directory=\"my_baysian_choral\", project_name=\"baysian_opt\"\n",
    ")\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "baysian_opt_tuner.search(\n",
    "    train_ds.take(100), epochs=10,\n",
    "    validation_data=valid_ds.take(100),\n",
    "    callbacks=[early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_conv_layers': 3,\n",
       " 'n_recurrent_layers': 1,\n",
       " 'learning_rate': 0.000303917691197622}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baysian_opt_tuner.get_best_hyperparameters(1)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_conv_layers=3, n_recurrent_layers=1)\n",
    "\n",
    "learning_rate = baysian_opt_tuner.get_best_hyperparameters(1)[0].values[\"learning_rate\"]\n",
    "optimizer = tf.keras.optimizers.legacy.Nadam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, iterations:int, max_lr:float=1e-3, start_lr:float=None, last_iterations:int=None, last_lr:float=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10.0\n",
    "        self.last_iterations = last_iterations or ((iterations // 10) + 1)\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or (self.start_lr / 1e3)\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def _interpolate(self, iter1:int, iter2:int, lr1:float, lr2:float) -> float:\n",
    "        slope = (lr2 - lr1) / (iter2 - iter1)\n",
    "        return slope * (self.iteration - iter1) + lr1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0, self.half_iteration, self.start_lr, self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(self.half_iteration, 2*self.half_iteration, self.max_lr, self.start_lr)\n",
    "        else:\n",
    "            lr = self._interpolate(2*self.half_iteration, self.iterations, self.start_lr, self.last_lr)\n",
    "        \n",
    "        self.iteration += 1\n",
    "        tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1612/1612 [==============================] - 31s 18ms/step - loss: 2.9937 - accuracy: 0.3048 - val_loss: 2.0267 - val_accuracy: 0.5453\n",
      "Epoch 2/10\n",
      "1612/1612 [==============================] - 28s 18ms/step - loss: 1.4549 - accuracy: 0.6778 - val_loss: 1.1551 - val_accuracy: 0.7327\n",
      "Epoch 3/10\n",
      "1612/1612 [==============================] - 28s 18ms/step - loss: 0.9948 - accuracy: 0.7594 - val_loss: 0.9747 - val_accuracy: 0.7605\n",
      "Epoch 4/10\n",
      "1612/1612 [==============================] - 28s 18ms/step - loss: 0.8675 - accuracy: 0.7772 - val_loss: 0.8905 - val_accuracy: 0.7724\n",
      "Epoch 5/10\n",
      "1612/1612 [==============================] - 29s 18ms/step - loss: 0.8073 - accuracy: 0.7866 - val_loss: 0.8600 - val_accuracy: 0.7748\n",
      "Epoch 6/10\n",
      "1612/1612 [==============================] - 28s 17ms/step - loss: 0.7786 - accuracy: 0.7910 - val_loss: 0.8326 - val_accuracy: 0.7812\n",
      "Epoch 7/10\n",
      "1612/1612 [==============================] - 28s 18ms/step - loss: 0.7640 - accuracy: 0.7928 - val_loss: 0.8175 - val_accuracy: 0.7849\n",
      "Epoch 8/10\n",
      "1612/1612 [==============================] - 28s 18ms/step - loss: 0.7571 - accuracy: 0.7938 - val_loss: 0.8024 - val_accuracy: 0.7875\n",
      "Epoch 9/10\n",
      "1612/1612 [==============================] - 29s 18ms/step - loss: 0.7562 - accuracy: 0.7934 - val_loss: 0.7844 - val_accuracy: 0.7911\n",
      "Epoch 10/10\n",
      "1612/1612 [==============================] - 28s 18ms/step - loss: 0.7583 - accuracy: 0.7927 - val_loss: 0.7798 - val_accuracy: 0.7936\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "oneCycle = OneCycleScheduler(\n",
    "    iterations=1612*n_epochs,\n",
    "    max_lr=learning_rate\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, epochs=n_epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[oneCycle]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_bach_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_bach_rnn/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_bach_rnn\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_scaled_notes(scaled_notes: tf.Tensor):\n",
    "    notes = tf.where(scaled_notes == 0, scaled_notes, scaled_notes + MIN_VAL - 1)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale(model: tf.keras.Model, seed_notes: tf.Tensor, n_chords: int, temperature: float=1.0):\n",
    "    N_NOTES_PER_CHORD = 4\n",
    "    arpegio = tf.cast(seed_notes, tf.int64)\n",
    "    arpegio = tf.reshape(arpegio, shape=[1, -1])\n",
    "    for chord_idx in range(n_chords):\n",
    "        for note_idx in range(N_NOTES_PER_CHORD):\n",
    "            next_note_probas = model(arpegio, training=False)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = revert_scaled_notes(arpegio)\n",
    "    chorale = tf.reshape(arpegio, [-1, N_NOTES_PER_CHORD])\n",
    "    return chorale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_notes(notes: tf.Tensor):\n",
    "    notes_scaled = tf.where(notes == 0, notes, notes - MIN_VAL + 1)\n",
    "    return notes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_chorale(chorale: tf.Tensor):\n",
    "    memFile = BytesIO()\n",
    "    MyMIDI = MIDIFile(1)\n",
    "\n",
    "    track = 0\n",
    "    time = 0\n",
    "    channel = 0\n",
    "    duration = 1\n",
    "    volume = 100\n",
    "    MyMIDI.addTrackName(track,time,\"Sample Track\")\n",
    "    MyMIDI.addTempo(track,time,240)\n",
    "\n",
    "    # WRITE A SCALE\n",
    "    for chord in chorale:\n",
    "        for note in chord:\n",
    "            MyMIDI.addNote(track, channel, note, time, duration, volume)\n",
    "        time += duration\n",
    "    MyMIDI.writeFile(memFile)\n",
    "\n",
    "    # PLAYBACK\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    memFile.seek(0)\n",
    "    pygame.mixer.music.load(memFile)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in test_ds.shuffle(100, seed=41).take(1):\n",
    "    N_EXTRA_NOTES = 3 # There are always 3 more notes than an even multiple of 4 because of the windowing.\n",
    "    seed_notes = X[0][:-N_EXTRA_NOTES]\n",
    "    n_chords = 32\n",
    "    new_chorale = generate_chorale(model, seed_notes, n_chords, temperature=.9)\n",
    "    play_chorale(new_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
